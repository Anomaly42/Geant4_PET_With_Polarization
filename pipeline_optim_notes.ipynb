{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pipeline\n",
    "\n",
    "There are clearly two parts to the entire pipeline:\n",
    "1. Event data generation by GEANT4\n",
    "2. Analysis with python scripts\n",
    "\n",
    "On the CPU, event generation works at ~50,000 events / second  \n",
    "which for 1 billion events requires 20,000 seconds or ~6 hours.\n",
    "\n",
    "Since reproducing the plot of interest (fig 2 from Polarization enhanced X-ray imaging for biomedicine) requires data generation upto 5 billion events at a time, this easily runs into a temporal bottleneck.\n",
    "\n",
    "The Laptop I am currently using to analyze the data has the maximum RAM of 16 GB. Since the 1e9 dataset is ~15.4 GB, loading the dataset causes the script to crash for raw data larger than the 5e8 event ROOT file.\n",
    "\n",
    "A way arround this memory problem is simply to process the data in batches. In the (unbatched) pipeline, all data columns are loadedn and rearranged such that time_ns is in sorted order. Then we extract the coincidence events from time_ns alone and process the coincidence events. Coincidence event processing (after computation of the time_window_coincidences array) can be processed in batches.\n",
    "\n",
    "_In fact_, this processing can be done using the GPU!\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
